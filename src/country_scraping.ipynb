{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "from tqdm.notebook import tqdm\n",
    "#pip install --upgrade google-api-python-client\n",
    "#pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "\n",
    "import requests as req\n",
    "#from bs4 import BeautifulSoup as bs\n",
    "\n",
    "sys.path.insert(0, '..') # project folder\n",
    "path_data = op.join('..', 'data', 'raw')\n",
    "path_deriv = op.join(path_data, '..', 'derivatives')\n",
    "path_metadata = op.join(path_data, \"yt_metadata_en.jsonl.gz\")\n",
    "path_edu = op.join(path_deriv, \"Education_videos_{}.csv\")\n",
    "\n",
    "API_KEY = pd.read_json(op.join('.','config.json'))['api_key'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file : path_edu_0  --> Found 3412 channels\n",
      "Processing file : path_edu_1  --> Found 3039 channels\n",
      "Processing file : path_edu_2  --> Found 3069 channels\n",
      "Processing file : path_edu_3  --> Found 3036 channels\n",
      "Processing file : path_edu_4  --> Found 3384 channels\n",
      "Processing file : path_edu_5  --> Found 3282 channels\n",
      "Processing file : path_edu_6  --> Found 3150 channels\n",
      "Processing file : path_edu_7  --> Found 3224 channels\n",
      "Total number of channels : 25596\n"
     ]
    }
   ],
   "source": [
    "def extract_channels_edu(verbose = False):\n",
    "    channels = []\n",
    "    for i in range(8):\n",
    "            if verbose :\n",
    "                print(f'Processing file : path_edu_{i}', end = '')\n",
    "            edu = pd.read_csv(path_edu.format(i), index_col=0)\n",
    "            ch = list(pd.unique(edu['channel_id']))\n",
    "            if verbose : \n",
    "                print(f\"  --> Found {len(ch)} channels\")\n",
    "            channels.extend(ch)\n",
    "        \n",
    "    if verbose:\n",
    "         print('Total number of channels :' , len(channels))\n",
    "    return channels\n",
    "\n",
    "channels = extract_channels_edu(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "def youtube_country_scraper(api_key, channel_ids, verbose = False, redo = False):\n",
    "    # Disable OAuthlib's HTTPS verification when running locally. *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "    youtube = build('youtube', 'v3', developerKey = api_key)\n",
    "    ids_string = \",\".join(channel_ids)\n",
    "    \n",
    "    request = youtube.channels().list(\n",
    "        part = 'snippet',\n",
    "        id= ids_string\n",
    "    )\n",
    "    items = request.execute()\n",
    "    countries = {ch: 'Redo' for ch in channel_ids}\n",
    "    if ('items' in items):\n",
    "        for item in items.get('items', []):\n",
    "            if 'snippet' in item:\n",
    "                id = item.get('id')\n",
    "                country  = item.get('snippet').get('country')\n",
    "                if (id in channel_ids): # else the channel now has a different id and need to be redone\n",
    "                    countries[id] = country\n",
    "            else:\n",
    "                countries[id] = None\n",
    "    else:\n",
    "        countries[list(countries)[0]] = 'deleted' # channel info is not available anymore\n",
    "    if verbose :\n",
    "        print(items)\n",
    "        print(countries)\n",
    "    return countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.DataFrame(columns = ['channel_id', 'country']) \n",
    "countries['channel_id'] = channels\n",
    "countries['country'] = 'empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if op.isfile(op.join(path_deriv, 'countries.csv')):\n",
    "    countries = pd.read_csv(op.join(path_deriv, 'countries.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = len(countries) # max 10k per day - improved since now we do batches so can run all of them in one go\n",
    "batch_size = 50 # max youtube allows per request\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start,end,batch_size), total = (end - start)//batch_size, desc = 'Country extraction'):\n",
    "        chs = channels[i:min(i+batch_size,end)]\n",
    "        nations = youtube_country_scraper(API_KEY, chs, verbose= False)\n",
    "        \n",
    "        for ch in chs:\n",
    "            countries.loc[countries.channel_id == ch, 'country'] = nations[ch]\n",
    "            tqdm.write('Channel {} : {} - {}  '.format(i, ch, nations[ch]), end=\"\\r\")\n",
    "\n",
    "    if end % batch_size != 0:\n",
    "        chs_extra = channels[(end//batch_size)* batch_size:end]\n",
    "        nations = youtube_country_scraper(API_KEY, chs_extra, verbose= False)\n",
    "        for i, ch in enumerate(chs_extra):\n",
    "            countries.loc[countries.channel_id == ch, 'country'] = nations[ch]\n",
    "            tqdm.write('Channel {} : {} - {}  '.format(i, ch, nations[ch]), end=\"\\r\")\n",
    "\n",
    "except HttpError as e:\n",
    "        if e.resp.status == 403:\n",
    "            print('Quota exceeded, saving extracted countries')\n",
    "            countries.to_csv(op.join(path_deriv, 'countries.csv'), index = False)        \n",
    "\n",
    "countries.to_csv(op.join(path_deriv, 'countries.csv'))\n",
    "tqdm.write('Done!                                        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36759446a31d4f40b2dca96c10f48a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Countries redo:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel 1249 : UCrxytzQWXftMcG-2bLs2Ubg - deleted  \r"
     ]
    }
   ],
   "source": [
    "#?????\n",
    "#countries.loc[countries.country == 'deleted', 'country'] = 'Redo'\n",
    "#Redo those that did not return the same channel id\n",
    "countries_redo = countries[countries.country == 'Redo']\n",
    "for i, ch in tqdm(enumerate(countries_redo.channel_id), total = len(countries_redo), desc = 'Countries redo'):\n",
    "    nations = youtube_country_scraper(API_KEY, [ch], verbose= False)\n",
    "    countries.loc[countries.channel_id == ch, 'country'] = nations[ch]\n",
    "    tqdm.write('Channel {} : {} - {}  '.format(i, ch, nations[ch]), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_csv(op.join(path_deriv, 'countries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "US         11632\n",
       "None        3622\n",
       "IN          2452\n",
       "GB          1873\n",
       "deleted     1250\n",
       "CA          1092\n",
       "AU           551\n",
       "PK           249\n",
       "DE           231\n",
       "PH           151\n",
       "NL           127\n",
       "SE           110\n",
       "NZ            97\n",
       "BD            97\n",
       "ID            93\n",
       "FR            92\n",
       "ES            88\n",
       "IE            79\n",
       "IT            78\n",
       "SG            76\n",
       "JP            68\n",
       "CH            65\n",
       "ZA            63\n",
       "RO            56\n",
       "AE            56\n",
       "NP            55\n",
       "MY            53\n",
       "PL            53\n",
       "DK            50\n",
       "KR            47\n",
       "RU            42\n",
       "NO            40\n",
       "BR            38\n",
       "BE            38\n",
       "TH            37\n",
       "AT            37\n",
       "PT            35\n",
       "UA            34\n",
       "FI            32\n",
       "NG            31\n",
       "HK            30\n",
       "MX            29\n",
       "IL            29\n",
       "VN            27\n",
       "KE            26\n",
       "GR            25\n",
       "CZ            23\n",
       "HR            21\n",
       "HU            21\n",
       "LK            20\n",
       "RS            20\n",
       "SI            19\n",
       "SA            17\n",
       "TR            17\n",
       "SK            16\n",
       "KH            15\n",
       "TW            13\n",
       "GH            13\n",
       "CO            12\n",
       "LV            11\n",
       "BG            11\n",
       "JM            11\n",
       "MA            11\n",
       "CY            10\n",
       "AR            10\n",
       "CN            10\n",
       "EG            10\n",
       "DZ             9\n",
       "EE             9\n",
       "CL             7\n",
       "MT             7\n",
       "IQ             6\n",
       "QA             6\n",
       "CR             6\n",
       "LT             5\n",
       "UG             5\n",
       "TZ             5\n",
       "KZ             5\n",
       "BY             4\n",
       "PE             4\n",
       "BA             4\n",
       "KW             4\n",
       "OM             4\n",
       "EC             3\n",
       "BH             3\n",
       "LU             3\n",
       "JO             3\n",
       "ZW             3\n",
       "LB             3\n",
       "PR             3\n",
       "MK             3\n",
       "TT             2\n",
       "IM             2\n",
       "IS             2\n",
       "AQ             2\n",
       "AL             2\n",
       "BB             2\n",
       "PA             2\n",
       "MD             2\n",
       "GE             2\n",
       "FK             1\n",
       "AI             1\n",
       "CX             1\n",
       "IR             1\n",
       "VE             1\n",
       "SN             1\n",
       "VI             1\n",
       "AD             1\n",
       "GT             1\n",
       "ME             1\n",
       "BM             1\n",
       "AZ             1\n",
       "BO             1\n",
       "AF             1\n",
       "BS             1\n",
       "SJ             1\n",
       "AO             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None) #11 is default\n",
    "countries.country.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_csv(op.join(path_deriv, 'countries.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other explored methods - legacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"url = r'https://socialblade.com/youtube/c/simonegiertz'\n",
    "r = req.get(url)\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "soup = bs(r.text, 'html.parser')\n",
    "country = soup.find('span', {'id': 'youtube-stats-header-country'}).text\n",
    "print(country)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200\n",
      "\n",
      "US\n"
     ]
    }
   ],
   "source": [
    "#Web archive for Socialblade - solution not practical\n",
    "\"\"\"url = r'https://web.archive.org/web/20161218062757/https://socialblade.com/youtube/user/leafyishere/monthly'\n",
    "r = req.get(url)\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "soup = bs(r.text, 'html.parser')\n",
    "country = soup.find('span', {'id': 'youtube-stats-header-country'}).text\n",
    "print(country)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
