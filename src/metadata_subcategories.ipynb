{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "\n",
    "sys.path.insert(0, '..') # project folder\n",
    "path_data = op.join('..', 'data', 'raw')\n",
    "path_metadata = op.join(path_data, \"yt_metadata_en.jsonl.gz\")\n",
    "path_channels = op.join(path_data, \"df_channels_en.tsv.gz\")\n",
    "path_deriv = op.join(path_data, '..', 'derivatives')\n",
    "path_edu = op.join(path_deriv, \"Education_videos_{}.csv\")\n",
    "path_edu_clean = op.join(path_deriv, \"Education_videos_{}clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels = pd.read_csv(path_channels, compression=\"infer\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels = pd.read_csv(path_channels, compression=\"infer\", sep=\"\\t\")\n",
    "df_channels[\"join_date\"] = pd.to_datetime(df_channels[\"join_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>2006-09-01</td>\n",
       "      <td>UCbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>Cocomelon - Nursery ...</td>\n",
       "      <td>60100000</td>\n",
       "      <td>458</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Education</td>\n",
       "      <td>2013-02-09</td>\n",
       "      <td>UCBnZ16ahKA2DZ_T5W0FPUXg</td>\n",
       "      <td>ChuChu TV Nursery Rh...</td>\n",
       "      <td>26600000</td>\n",
       "      <td>317</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Education</td>\n",
       "      <td>2011-12-14</td>\n",
       "      <td>UCcdwLMPsaU2ezNSJU1nFoBQ</td>\n",
       "      <td>Pinkfong! Kids' Song...</td>\n",
       "      <td>23200000</td>\n",
       "      <td>1382</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Education</td>\n",
       "      <td>2013-10-25</td>\n",
       "      <td>UCRx3mKNUdl8QE06nEug7p6Q</td>\n",
       "      <td>Billion Surprise Toy...</td>\n",
       "      <td>24700000</td>\n",
       "      <td>150</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Education</td>\n",
       "      <td>2011-06-22</td>\n",
       "      <td>UCKAqou7V9FAWXpZd9xtOg3Q</td>\n",
       "      <td>Little Baby Bum - Nu...</td>\n",
       "      <td>19900000</td>\n",
       "      <td>958</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136282</th>\n",
       "      <td>Education</td>\n",
       "      <td>2012-11-21</td>\n",
       "      <td>UCccoliC479T88ziS1TxeLeg</td>\n",
       "      <td>skunkpaste</td>\n",
       "      <td>10074</td>\n",
       "      <td>146</td>\n",
       "      <td>977576.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136291</th>\n",
       "      <td>Education</td>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>UCu13MNNOLhyRV-YtlYWV4tw</td>\n",
       "      <td>Matthew Williams</td>\n",
       "      <td>10000</td>\n",
       "      <td>374</td>\n",
       "      <td>977915.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136316</th>\n",
       "      <td>Education</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>UCRoZf8DdJzF5Xi99gtFDY0A</td>\n",
       "      <td>Geoid</td>\n",
       "      <td>10276</td>\n",
       "      <td>136</td>\n",
       "      <td>979269.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136332</th>\n",
       "      <td>Education</td>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>UCax581uo5yNzdgODBdH67cw</td>\n",
       "      <td>Bond Robin</td>\n",
       "      <td>10225</td>\n",
       "      <td>469</td>\n",
       "      <td>980820.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136383</th>\n",
       "      <td>Education</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>UC68hdd5dd1zJAjxJq2Jz7rw</td>\n",
       "      <td>Nour Saidana</td>\n",
       "      <td>10506</td>\n",
       "      <td>74</td>\n",
       "      <td>988795.0</td>\n",
       "      <td>53.1435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7803 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_cc  join_date                   channel  \\\n",
       "1        Education 2006-09-01  UCbCmjCuTUZos6Inko4u57UQ   \n",
       "22       Education 2013-02-09  UCBnZ16ahKA2DZ_T5W0FPUXg   \n",
       "27       Education 2011-12-14  UCcdwLMPsaU2ezNSJU1nFoBQ   \n",
       "39       Education 2013-10-25  UCRx3mKNUdl8QE06nEug7p6Q   \n",
       "46       Education 2011-06-22  UCKAqou7V9FAWXpZd9xtOg3Q   \n",
       "...            ...        ...                       ...   \n",
       "136282   Education 2012-11-21  UCccoliC479T88ziS1TxeLeg   \n",
       "136291   Education 2013-09-05  UCu13MNNOLhyRV-YtlYWV4tw   \n",
       "136316   Education 2015-08-12  UCRoZf8DdJzF5Xi99gtFDY0A   \n",
       "136332   Education 2013-09-10  UCax581uo5yNzdgODBdH67cw   \n",
       "136383   Education 2016-03-21  UC68hdd5dd1zJAjxJq2Jz7rw   \n",
       "\n",
       "                        name_cc  subscribers_cc  videos_cc  \\\n",
       "1       Cocomelon - Nursery ...        60100000        458   \n",
       "22      ChuChu TV Nursery Rh...        26600000        317   \n",
       "27      Pinkfong! Kids' Song...        23200000       1382   \n",
       "39      Billion Surprise Toy...        24700000        150   \n",
       "46      Little Baby Bum - Nu...        19900000        958   \n",
       "...                         ...             ...        ...   \n",
       "136282               skunkpaste           10074        146   \n",
       "136291         Matthew Williams           10000        374   \n",
       "136316                    Geoid           10276        136   \n",
       "136332               Bond Robin           10225        469   \n",
       "136383             Nour Saidana           10506         74   \n",
       "\n",
       "        subscriber_rank_sb  weights  \n",
       "1                      7.0   2.0870  \n",
       "22                    44.0   2.0870  \n",
       "27                    55.0   2.0870  \n",
       "39                    83.0   2.0870  \n",
       "46                    93.0   2.0870  \n",
       "...                    ...      ...  \n",
       "136282            977576.0  53.1435  \n",
       "136291            977915.0  53.1435  \n",
       "136316            979269.0  53.1435  \n",
       "136332            980820.0  53.1435  \n",
       "136383            988795.0  53.1435  \n",
       "\n",
       "[7803 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edu = df_channels.loc[df_channels['category_cc'].isin(['Education'])]\n",
    "df_edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire des sous catégories dans education, \n",
    "# plot : Camembert avec les differentes sous categories, rayon est le nombre total \n",
    "# how to - match channels with their videos and try to get the theme of them by using the description, title and tags fiels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction youtube metadata (legacy - see code Viva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3795564 videos in the Education category!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "def filter_jsonl(input, category, batch_size, random_seed, all = False):\n",
    "    filtered_data = []\n",
    "    random.seed(random_seed)\n",
    "    counter = 0\n",
    "    with gzip.open(input, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            if entry.get('categories') == category:\n",
    "                counter +=1\n",
    "                if len(filtered_data) <= batch_size or all:\n",
    "                    filtered_data.append(entry)\n",
    "                else:\n",
    "                    index_to_replace = random.randint(0, len(filtered_data) - 1)\n",
    "                    if index_to_replace < batch_size:\n",
    "                        filtered_data[index_to_replace] = entry\n",
    "      \n",
    "                \n",
    "    print(f\"There are {counter} videos in the Education category!\")\n",
    "    return pd.DataFrame(filtered_data)\n",
    "\n",
    "            \n",
    "\n",
    "df = filter_jsonl(path_metadata, 'Education', 500000, 0, all = False)\n",
    "\n",
    "df.to_csv(op.join(path_deriv, 'df_edu_all.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of video descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "i = 0\n",
    "df = pd.read_csv(op.join(path_deriv, path_edu.format(i)), index_col=0) #pd.read_csv(op.join(path_deriv, 'df_edu_all.csv'), index_col=0)\n",
    "df['desc_clean'] = df['description'].fillna('')\n",
    "df['text'] = df['title'].fillna('') + \" \" + df['tags'].fillna('') \n",
    "\n",
    "#sort the description separately since way more unrelated text, some titles contain keywords we remove in descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_clean</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34620</th>\n",
       "      <td>A video, that describes, who, I believe, Jesus, is, to me.</td>\n",
       "      <td>Who Jesus Is To Me   (Automated Voice) God’sTruthBeingTold,who Jesus is to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35217</th>\n",
       "      <td>Tim Crane gives a general overview of analytic philosophy and its nature, fo...</td>\n",
       "      <td>Analytic Philosophy: Its History, Science, &amp; Logic Philosophy,Analytic Philo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35816</th>\n",
       "      <td>Get Free Korean Lessons on your Android, iPhone, iPad or Kindle Fire! Click ...</td>\n",
       "      <td>Learn the Top 25 Korean Adjectives Korean Language (Human Language),learn Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35291</th>\n",
       "      <td>Please thank Lucky Gunner for bringing us today’s video of VangComp Makes Ex...</td>\n",
       "      <td>VangComp Makes Excellent Shotguns asp,cover your asp,active self protection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10905</th>\n",
       "      <td>Reviewing the EQ 73 and EQ 81 plugins from IK Multimedia.</td>\n",
       "      <td>IK Multimedia EQ 73 &amp; EQ 81 Review - Audio Plugin IK Multimedia (Business Op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            desc_clean  \\\n",
       "34620                       A video, that describes, who, I believe, Jesus, is, to me.   \n",
       "35217  Tim Crane gives a general overview of analytic philosophy and its nature, fo...   \n",
       "35816  Get Free Korean Lessons on your Android, iPhone, iPad or Kindle Fire! Click ...   \n",
       "35291  Please thank Lucky Gunner for bringing us today’s video of VangComp Makes Ex...   \n",
       "10905                        Reviewing the EQ 73 and EQ 81 plugins from IK Multimedia.   \n",
       "\n",
       "                                                                                  text  \n",
       "34620  Who Jesus Is To Me   (Automated Voice) God’sTruthBeingTold,who Jesus is to m...  \n",
       "35217  Analytic Philosophy: Its History, Science, & Logic Philosophy,Analytic Philo...  \n",
       "35816  Learn the Top 25 Korean Adjectives Korean Language (Human Language),learn Ko...  \n",
       "35291  VangComp Makes Excellent Shotguns asp,cover your asp,active self protection,...  \n",
       "10905  IK Multimedia EQ 73 & EQ 81 Review - Audio Plugin IK Multimedia (Business Op...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 80)\n",
    "df.sample(5)[['desc_clean', 'text']]\n",
    "# some examples of problematic data\n",
    "# 32379 crypto video with lots of links\n",
    "# 35962 marseillaise\n",
    "# 22678 41830 non alpha and indian\n",
    "# 34547 numbers accolated\n",
    "# 17337 very long strings in crypto vids\n",
    "# 3235 hindu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['title'].str.contains('twitter')]['title']\n",
    "\n",
    "\"\"\"string = df['description'].iloc[5046].lower()\n",
    "print(string)\n",
    "remove_urls(string)#.replace('website', 'website blab bla truc muche promotion')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything to lower case so that same words are treated the same\n",
    "df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "df['desc_clean'] = df['desc_clean'].apply(lambda x: x.lower() if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls links and their associated text\n",
    "url_pattern = re.compile(r'(https?://\\S+|www\\.\\S+)') \n",
    "com_pattern = re.compile(r'([^\\s]+\\.com)')\n",
    "fb_pattern = re.compile(r'(facebook\\s+page|facebook\\s+group)[^\\w\\s]*.*?(\\n|$)')\n",
    "#lines that start with brands or websites \n",
    "link_pattern = re.compile(r'(\\n+)(facebook|twitter|pinterest|tumblr|instagram|website|amazon)[^\\w\\s]*\\s+\\S+')\n",
    "link_pattern2 = re.compile(r'(\\n+)(facebook|twitter|pinterest|tumblr|instagram|website|amazon)')\n",
    "\n",
    "#TODO lines that contain these websites in the middle of text - hard to implement since not sure if discard\n",
    "\n",
    "long_words = re.compile(r'\\b[a-zA-Z0-9]{21,}\\b') # most words in english are below 20 letters, bigger than that is a crypto wallet id\n",
    "\n",
    "def remove_urls(text, desc = False):\n",
    "        text = url_pattern.sub('', text)\n",
    "        text = com_pattern.sub('', text)\n",
    "        text = long_words.sub('', text)\n",
    "        if desc: # titles and tags should keep brand names since these might be the focus of the video\n",
    "                text = fb_pattern.sub('', text)\n",
    "                text = link_pattern.sub('', text)\n",
    "                return link_pattern2.sub('', text)\n",
    "        return text\n",
    "\n",
    "df['desc_clean'] = df['desc_clean'].apply(remove_urls, desc = True)\n",
    "df['text'] = df['text'].apply(remove_urls)\n",
    "#test = 'https://www.youtube.com/everydaytacticalvids\\n my twitter account - https://twitter.com/everydaytactic1\\n my facebook group tha tha \\n ow'\n",
    "#remove_urls(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_word(text): #punctuation, underscores seem to evade this regex so add it\n",
    "        return  re.sub(r'[^\\w\\s]|_+', ' ', text)\n",
    "\n",
    "def clean_non_ascii(text): # indian symbols that might still be left\n",
    "        return  re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "df['desc_clean'] = df['desc_clean'].apply(clean_non_word).apply(clean_non_ascii)\n",
    "df['text'] = df['text'].apply(clean_non_word).apply(clean_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric(text):\n",
    "    return re.sub(r'\\d+(?![a-zA-Z])', '', text) # numbers that are not accolated to strings : # TODO MAybe all numbers\n",
    "df['desc_clean'] = df['desc_clean'].apply(clean_numeric)\n",
    "df['text'] = df['text'].apply(clean_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_space_newline(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text.replace('\\n', ' ')).strip()\n",
    "\n",
    "df['desc_clean'] = df['desc_clean'].apply(clean_space_newline)\n",
    "df['text'] = df['text'].apply(clean_space_newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine both \n",
    "df['text_clean'] = df['text'] + \" \" + df['desc_clean']\n",
    "df = df.drop(['text', 'desc_clean'], axis = 1)\n",
    "df.to_csv(op.join(path_deriv, path_edu_clean.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional : make the dataframe smaller \n",
    "df = df.drop(['description'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize subtopics - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to perform lemmatization, stemming or stopword removal since BERT handles it \n",
    "#df= pd.read_csv(op.join(path_deriv, 'df_edu_500k_clean.csv'))\n",
    "df= pd.read_csv(op.join(path_deriv, path_edu_clean.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = df.copy().sample(50000)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "trial = trial.reset_index(drop=True)\n",
    "trial.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Encoding model...\n"
     ]
    }
   ],
   "source": [
    "# Try to extract sub topics with sentence transformers like BERT \n",
    "from sentence_transformers import SentenceTransformer # https://sbert.net/\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print('1. Encoding model...')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "embeddings = model.encode(trial.get('text_clean'))\n",
    "np.save(op.join(path_deriv, 'embeddings.npy'), embeddings) # 41 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Starting kmeans...\n"
     ]
    }
   ],
   "source": [
    "print('2. Starting kmeans...')\n",
    "num_clusters = 50\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Fitting k means...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\gbrag\\miniconda3\\envs\\ada\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "print('3. Fitting k means...') # 10s\n",
    "clusters = kmeans.fit(embeddings)\n",
    "cluster_assignment = clusters.labels_\n",
    "print('4. Predicting labels...')\n",
    "trial['cluster'] = clusters.predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_num in range(num_clusters):\n",
    "    print(f\"Cluster {cluster_num}\")\n",
    "    cluster_data = trial[trial['cluster'] == cluster_num]\n",
    "    display(cluster_data.sample(5)[['title', 'text_clean', 'tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, 121 videos: asl.sign.language\n",
      "Cluster 1, 1601 videos: ca.gk.learning\n",
      "Cluster 2, 1833 videos: trp.tv.live\n",
      "Cluster 3, 1137 videos: video.youtube.use\n",
      "Cluster 4, 451 videos: sunday.adelaja.pastor\n",
      "Cluster 5, 867 videos: tedx.ted.organized\n",
      "Cluster 6, 1231 videos: money.marketing.online\n",
      "Cluster 7, 1199 videos: maths.examsolutions.math\n",
      "Cluster 8, 659 videos: food.bajias.cooking\n",
      "Cluster 9, 1161 videos: garden.house.gardening\n",
      "Cluster 10, 1388 videos: kids.cbeebies.learn\n",
      "Cluster 11, 956 videos: rhymes.songs.nursery\n",
      "Cluster 12, 2153 videos: excel.data.video\n",
      "Cluster 13, 1403 videos: english.learn.japanese\n",
      "Cluster 14, 1263 videos: dr.health.medical\n",
      "Cluster 15, 975 videos: life.health.mental\n",
      "Cluster 16, 1331 videos: news.video.use\n",
      "Cluster 17, 984 videos: business.management.iese\n",
      "Cluster 18, 1145 videos: chess.history.course\n",
      "Cluster 19, 1620 videos: tips.remedies.awesome\n",
      "Cluster 20, 287 videos: san.diego.air\n",
      "Cluster 21, 1004 videos: survival.asp.knife\n",
      "Cluster 22, 841 videos: trading.day.market\n",
      "Cluster 23, 1019 videos: earth.flat.world\n",
      "Cluster 24, 1265 videos: air.jordan.camera\n",
      "Cluster 25, 981 videos: adda.ssc.po\n",
      "Cluster 26, 1286 videos: tutorial.course.programming\n",
      "Cluster 27, 1125 videos: mean.does.meaning\n",
      "Cluster 28, 993 videos: swami.chinmaya.india\n",
      "Cluster 29, 626 videos: lecture.engineering.lec\n",
      "Cluster 30, 412 videos: astrology.reading.love\n",
      "Cluster 31, 843 videos: chemistry.biology.science\n",
      "Cluster 32, 1465 videos: guitar.piano.music\n",
      "Cluster 33, 109 videos: iqra.bangla.al\n",
      "Cluster 34, 872 videos: reaction.kabbalah.law\n",
      "Cluster 35, 1116 videos: god.sermon.bible\n",
      "Cluster 36, 420 videos: cc.licensed.ba\n",
      "Cluster 37, 467 videos: vu.topic.course\n",
      "Cluster 38, 1737 videos: photoshop.art.tutorial\n",
      "Cluster 39, 1382 videos: hindi.avyakt.bapdada\n",
      "Cluster 40, 1186 videos: dr.health.yoga\n",
      "Cluster 41, 1257 videos: university.vanderbilt.school\n",
      "Cluster 42, 545 videos: god.kingdom.prophecy\n",
      "Cluster 43, 756 videos: hebrew.god.bible\n",
      "Cluster 44, 599 videos: la.en.el\n",
      "Cluster 45, 1215 videos: shaykh.zakir.islam\n",
      "Cluster 46, 628 videos: bitcoin.silver.coin\n",
      "Cluster 47, 324 videos: brahmakumaris.peace.spiritual\n",
      "Cluster 48, 890 videos: philosophy.science.university\n",
      "Cluster 49, 872 videos: art.design.arts\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cluster_dict = {}\n",
    "\n",
    "for cluster in range(kmeans.n_clusters): #\n",
    "    texts = [row['text_clean'] for _, row in trial.iterrows() if row['cluster'] == cluster]\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=1, random_state=1) # only one category per cluster\n",
    "    lda.fit(tfidf_matrix)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_idx = lda.components_[0].argsort()[-3:][::-1]\n",
    "    \n",
    "    terms = [feature_names[i] for i in top_idx]\n",
    "    cluster_dict[cluster] = '.'.join(terms)\n",
    "    print(f\"Cluster {cluster}, {len(texts)} videos: {'.'.join(terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial['cluster_name'] = trial['cluster'].map(cluster_dict)\n",
    "trial.to_csv(op.join(path_deriv, 'trial_clustered_50k.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 121, 1601, 1833, 1137,  451,  867, 1231, 1199,  659, 1161, 1388,\n",
       "        956, 2153, 1403, 1263,  975, 1331,  984, 1145, 1620,  287, 1004,\n",
       "        841, 1019, 1265,  981, 1286, 1125,  993,  626,  412,  843, 1465,\n",
       "        109,  872, 1116,  420,  467, 1737, 1382, 1186, 1257,  545,  756,\n",
       "        599, 1215,  628,  324,  890,  872], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
