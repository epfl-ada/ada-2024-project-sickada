{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, '.') # add to path the current folder to use relative paths\n",
    "path_data = op.join('data', 'raw')\n",
    "path_metadata = op.join(path_data, \"yt_metadata_en.jsonl.gz\")\n",
    "path_channels = op.join(path_data, \"df_channels_en.tsv.gz\")\n",
    "path_deriv = op.join(path_data, '..', 'derivatives')\n",
    "path_edu = op.join(path_deriv, \"Education_videos_{}.csv\")\n",
    "path_edu_clean = op.join(path_deriv, \"Education_videos_{}clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels = pd.read_csv(path_channels, compression=\"infer\", sep=\"\\t\")\n",
    "df_channels[\"join_date\"] = pd.to_datetime(df_channels[\"join_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_cc</th>\n",
       "      <th>join_date</th>\n",
       "      <th>channel</th>\n",
       "      <th>name_cc</th>\n",
       "      <th>subscribers_cc</th>\n",
       "      <th>videos_cc</th>\n",
       "      <th>subscriber_rank_sb</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93193</th>\n",
       "      <td>Education</td>\n",
       "      <td>2013-03-11</td>\n",
       "      <td>UCa9r4ivs4mB3Qt2M2uODpEA</td>\n",
       "      <td>JP Gloria</td>\n",
       "      <td>15100</td>\n",
       "      <td>86</td>\n",
       "      <td>491113.0</td>\n",
       "      <td>10.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132100</th>\n",
       "      <td>Education</td>\n",
       "      <td>2007-10-11</td>\n",
       "      <td>UCwDlrvXecueebBA4vAwrDQQ</td>\n",
       "      <td>SCAD - The Savannah ...</td>\n",
       "      <td>10700</td>\n",
       "      <td>438</td>\n",
       "      <td>884463.0</td>\n",
       "      <td>11.9390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14488</th>\n",
       "      <td>Education</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>UC5bQ6WD_2NLGbfeJYIwAIuA</td>\n",
       "      <td>INKtalks</td>\n",
       "      <td>427021</td>\n",
       "      <td>480</td>\n",
       "      <td>40267.0</td>\n",
       "      <td>3.3105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87586</th>\n",
       "      <td>Education</td>\n",
       "      <td>2016-06-03</td>\n",
       "      <td>UChz_jz50dnm5IlYn6GQHAqg</td>\n",
       "      <td>kristyglassknits</td>\n",
       "      <td>26500</td>\n",
       "      <td>815</td>\n",
       "      <td>443193.0</td>\n",
       "      <td>7.2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90606</th>\n",
       "      <td>Education</td>\n",
       "      <td>2009-07-11</td>\n",
       "      <td>UCrBzGHKmGDcwLFnQGHJ3XYg</td>\n",
       "      <td>giant_neural_network...</td>\n",
       "      <td>24400</td>\n",
       "      <td>41</td>\n",
       "      <td>469286.0</td>\n",
       "      <td>8.0505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_cc  join_date                   channel  \\\n",
       "93193    Education 2013-03-11  UCa9r4ivs4mB3Qt2M2uODpEA   \n",
       "132100   Education 2007-10-11  UCwDlrvXecueebBA4vAwrDQQ   \n",
       "14488    Education 2011-07-07  UC5bQ6WD_2NLGbfeJYIwAIuA   \n",
       "87586    Education 2016-06-03  UChz_jz50dnm5IlYn6GQHAqg   \n",
       "90606    Education 2009-07-11  UCrBzGHKmGDcwLFnQGHJ3XYg   \n",
       "\n",
       "                        name_cc  subscribers_cc  videos_cc  \\\n",
       "93193                 JP Gloria           15100         86   \n",
       "132100  SCAD - The Savannah ...           10700        438   \n",
       "14488                  INKtalks          427021        480   \n",
       "87586          kristyglassknits           26500        815   \n",
       "90606   giant_neural_network...           24400         41   \n",
       "\n",
       "        subscriber_rank_sb  weights  \n",
       "93193             491113.0  10.1095  \n",
       "132100            884463.0  11.9390  \n",
       "14488              40267.0   3.3105  \n",
       "87586             443193.0   7.2380  \n",
       "90606             469286.0   8.0505  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edu = df_channels.loc[df_channels['category_cc'].isin(['Education'])]\n",
    "df_edu.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction youtube metadata (legacy - see chunk_video_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3795564 videos in the Education category!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "def filter_jsonl(input, category, batch_size, random_seed, all = False):\n",
    "    filtered_data = []\n",
    "    random.seed(random_seed)\n",
    "    counter = 0\n",
    "    with gzip.open(input, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            if entry.get('categories') == category:\n",
    "                counter +=1\n",
    "                if len(filtered_data) <= batch_size or all:\n",
    "                    filtered_data.append(entry)\n",
    "                else:\n",
    "                    index_to_replace = random.randint(0, len(filtered_data) - 1)\n",
    "                    if index_to_replace < batch_size:\n",
    "                        filtered_data[index_to_replace] = entry\n",
    "      \n",
    "                \n",
    "    print(f\"There are {counter} videos in the Education category!\")\n",
    "    return pd.DataFrame(filtered_data)\n",
    "\n",
    "            \n",
    "\n",
    "df = filter_jsonl(path_metadata, 'Education', 500000, 0, all = False)\n",
    "df.to_csv(op.join(path_deriv, 'df_edu_all.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of video descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # iterate over the 9 files\n",
    "df = pd.read_csv(op.join(path_deriv, path_edu.format(i)), index_col=0)\n",
    "\n",
    "# sort the description separately since way more unnecessary text\n",
    "# additionaly some titles contain keywords we remove in descriptions\n",
    "df['desc_clean'] = df['description'].fillna('')\n",
    "df['text'] = df['title'].fillna('') + \" \" + df['tags'].fillna('') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything to lower case so that same words are treated the same\n",
    "df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "df['desc_clean'] = df['desc_clean'].apply(lambda x: x.lower() if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls links and their associated text\n",
    "url_pattern = re.compile(r'(https?://\\S+|www\\.\\S+)') \n",
    "com_pattern = re.compile(r'([^\\s]+\\.com)')\n",
    "fb_pattern = re.compile(r'(facebook\\s+page|facebook\\s+group)[^\\w\\s]*.*?(\\n|$)')\n",
    "#lines that start with brands or websites \n",
    "link_pattern = re.compile(r'(\\n+)(facebook|twitter|pinterest|tumblr|instagram|website|amazon)[^\\w\\s]*\\s+\\S+')\n",
    "link_pattern2 = re.compile(r'(\\n+)(facebook|twitter|pinterest|tumblr|instagram|website|amazon)')\n",
    "\n",
    "#TODO lines that contain these websites in the middle of text - hard to implement since not sure if discard\n",
    "\n",
    "long_words = re.compile(r'\\b[a-zA-Z0-9]{21,}\\b') # most words in english are below 20 letters, bigger than that is a crypto wallet id\n",
    "\n",
    "def remove_urls(text, desc = False):\n",
    "        text = url_pattern.sub('', text)\n",
    "        text = com_pattern.sub('', text)\n",
    "        text = long_words.sub('', text)\n",
    "        if desc: # titles and tags should keep brand names since these might be the focus of the video\n",
    "                text = fb_pattern.sub('', text)\n",
    "                text = link_pattern.sub('', text)\n",
    "                return link_pattern2.sub('', text)\n",
    "        return text\n",
    "\n",
    "df['desc_clean'] = df['desc_clean'].apply(remove_urls, desc = True)\n",
    "df['text'] = df['text'].apply(remove_urls)\n",
    "#test = 'https://www.youtube.com/everydaytacticalvids\\n my twitter account - https://twitter.com/everydaytactic1\\n my facebook group tha tha \\n ow'\n",
    "#remove_urls(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_word(text): #punctuation, underscores seem to evade this regex so add it\n",
    "        return  re.sub(r'[^\\w\\s]|_+', ' ', text)\n",
    "\n",
    "def clean_non_ascii(text): # indian symbols that might still be left\n",
    "        return  re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "df['desc_clean'] = df['desc_clean'].apply(clean_non_word).apply(clean_non_ascii)\n",
    "df['text'] = df['text'].apply(clean_non_word).apply(clean_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric(text):\n",
    "    return re.sub(r'\\d+(?![a-zA-Z])', '', text) # numbers that are not accolated to strings : # TODO MAybe all numbers\n",
    "df['desc_clean'] = df['desc_clean'].apply(clean_numeric)\n",
    "df['text'] = df['text'].apply(clean_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_space_newline(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text.replace('\\n', ' ')).strip()\n",
    "\n",
    "df['desc_clean'] = df['desc_clean'].apply(clean_space_newline)\n",
    "df['text'] = df['text'].apply(clean_space_newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine both \n",
    "df['text_clean'] = df['text'] + \" \" + df['desc_clean']\n",
    "df = df.drop(['text', 'desc_clean'], axis = 1)\n",
    "df.to_csv(op.join(path_deriv, path_edu_clean.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional : make the dataframe smaller \n",
    "df = df.drop(['description'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['title'].str.contains('twitter')]['title']\n",
    "\n",
    "\"\"\"string = df['description'].iloc[5046].lower()\n",
    "print(string)\n",
    "remove_urls(string)#.replace('website', 'website blab bla truc muche promotion')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pd.set_option('display.max_colwidth', 80)\n",
    "df.sample(5)[['desc_clean', 'text']]\"\"\"\n",
    "# some examples of problematic data\n",
    "# 32379 crypto video with lots of links\n",
    "# 35962 marseillaise\n",
    "# 22678 41830 non alpha and indian\n",
    "# 34547 numbers accolated\n",
    "# 17337 very long strings in crypto vids\n",
    "# 3235 hindu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize subtopics - BERT (legacy see code Fred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to perform lemmatization, stemming or stopword removal since BERT handles it \n",
    "#df= pd.read_csv(op.join(path_deriv, 'df_edu_500k_clean.csv'))\n",
    "df= pd.read_csv(op.join(path_deriv, path_edu_clean.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = df.copy().sample(50000)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "trial = trial.reset_index(drop=True)\n",
    "trial.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Encoding model...\n"
     ]
    }
   ],
   "source": [
    "# Try to extract sub topics with sentence transformers like BERT \n",
    "from sentence_transformers import SentenceTransformer # https://sbert.net/\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print('1. Encoding model...')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "embeddings = model.encode(trial.get('text_clean'))\n",
    "np.save(op.join(path_deriv, 'embeddings.npy'), embeddings) # 41 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Starting kmeans...\n"
     ]
    }
   ],
   "source": [
    "print('2. Starting kmeans...')\n",
    "num_clusters = 50\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Fitting k means...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\gbrag\\miniconda3\\envs\\ada\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "print('3. Fitting k means...') # 10s\n",
    "clusters = kmeans.fit(embeddings)\n",
    "cluster_assignment = clusters.labels_\n",
    "print('4. Predicting labels...')\n",
    "trial['cluster'] = clusters.predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_num in range(num_clusters):\n",
    "    print(f\"Cluster {cluster_num}\")\n",
    "    cluster_data = trial[trial['cluster'] == cluster_num]\n",
    "    display(cluster_data.sample(5)[['title', 'text_clean', 'tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, 121 videos: asl.sign.language\n",
      "Cluster 1, 1601 videos: ca.gk.learning\n",
      "Cluster 2, 1833 videos: trp.tv.live\n",
      "Cluster 3, 1137 videos: video.youtube.use\n",
      "Cluster 4, 451 videos: sunday.adelaja.pastor\n",
      "Cluster 5, 867 videos: tedx.ted.organized\n",
      "Cluster 6, 1231 videos: money.marketing.online\n",
      "Cluster 7, 1199 videos: maths.examsolutions.math\n",
      "Cluster 8, 659 videos: food.bajias.cooking\n",
      "Cluster 9, 1161 videos: garden.house.gardening\n",
      "Cluster 10, 1388 videos: kids.cbeebies.learn\n",
      "Cluster 11, 956 videos: rhymes.songs.nursery\n",
      "Cluster 12, 2153 videos: excel.data.video\n",
      "Cluster 13, 1403 videos: english.learn.japanese\n",
      "Cluster 14, 1263 videos: dr.health.medical\n",
      "Cluster 15, 975 videos: life.health.mental\n",
      "Cluster 16, 1331 videos: news.video.use\n",
      "Cluster 17, 984 videos: business.management.iese\n",
      "Cluster 18, 1145 videos: chess.history.course\n",
      "Cluster 19, 1620 videos: tips.remedies.awesome\n",
      "Cluster 20, 287 videos: san.diego.air\n",
      "Cluster 21, 1004 videos: survival.asp.knife\n",
      "Cluster 22, 841 videos: trading.day.market\n",
      "Cluster 23, 1019 videos: earth.flat.world\n",
      "Cluster 24, 1265 videos: air.jordan.camera\n",
      "Cluster 25, 981 videos: adda.ssc.po\n",
      "Cluster 26, 1286 videos: tutorial.course.programming\n",
      "Cluster 27, 1125 videos: mean.does.meaning\n",
      "Cluster 28, 993 videos: swami.chinmaya.india\n",
      "Cluster 29, 626 videos: lecture.engineering.lec\n",
      "Cluster 30, 412 videos: astrology.reading.love\n",
      "Cluster 31, 843 videos: chemistry.biology.science\n",
      "Cluster 32, 1465 videos: guitar.piano.music\n",
      "Cluster 33, 109 videos: iqra.bangla.al\n",
      "Cluster 34, 872 videos: reaction.kabbalah.law\n",
      "Cluster 35, 1116 videos: god.sermon.bible\n",
      "Cluster 36, 420 videos: cc.licensed.ba\n",
      "Cluster 37, 467 videos: vu.topic.course\n",
      "Cluster 38, 1737 videos: photoshop.art.tutorial\n",
      "Cluster 39, 1382 videos: hindi.avyakt.bapdada\n",
      "Cluster 40, 1186 videos: dr.health.yoga\n",
      "Cluster 41, 1257 videos: university.vanderbilt.school\n",
      "Cluster 42, 545 videos: god.kingdom.prophecy\n",
      "Cluster 43, 756 videos: hebrew.god.bible\n",
      "Cluster 44, 599 videos: la.en.el\n",
      "Cluster 45, 1215 videos: shaykh.zakir.islam\n",
      "Cluster 46, 628 videos: bitcoin.silver.coin\n",
      "Cluster 47, 324 videos: brahmakumaris.peace.spiritual\n",
      "Cluster 48, 890 videos: philosophy.science.university\n",
      "Cluster 49, 872 videos: art.design.arts\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cluster_dict = {}\n",
    "\n",
    "for cluster in range(kmeans.n_clusters): #\n",
    "    texts = [row['text_clean'] for _, row in trial.iterrows() if row['cluster'] == cluster]\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=1, random_state=1) # only one category per cluster\n",
    "    lda.fit(tfidf_matrix)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_idx = lda.components_[0].argsort()[-3:][::-1]\n",
    "    \n",
    "    terms = [feature_names[i] for i in top_idx]\n",
    "    cluster_dict[cluster] = '.'.join(terms)\n",
    "    print(f\"Cluster {cluster}, {len(texts)} videos: {'.'.join(terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial['cluster_name'] = trial['cluster'].map(cluster_dict)\n",
    "trial.to_csv(op.join(path_deriv, 'trial_clustered_50k.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 121, 1601, 1833, 1137,  451,  867, 1231, 1199,  659, 1161, 1388,\n",
       "        956, 2153, 1403, 1263,  975, 1331,  984, 1145, 1620,  287, 1004,\n",
       "        841, 1019, 1265,  981, 1286, 1125,  993,  626,  412,  843, 1465,\n",
       "        109,  872, 1116,  420,  467, 1737, 1382, 1186, 1257,  545,  756,\n",
       "        599, 1215,  628,  324,  890,  872], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
